{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65nlzAA_Jobr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"aubmindlab/aragpt2-base\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4yTPmV_JbJi",
    "outputId": "0f592027-6a74-4a92-e08b-87c99a00b184"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DhTU8hvJe-A"
   },
   "outputs": [],
   "source": [
    "X_tr, X_val = train_test_split(df['text'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88ZGl-UIJk4U"
   },
   "outputs": [],
   "source": [
    "class Text:\n",
    "    def __init__(self, input_text, tokenizer, predict=False, decode=False):\n",
    "        if decode:\n",
    "            self.content = tokenizer.decode(input_text)\n",
    "            self.indexed_tokens = input_text\n",
    "        else:\n",
    "            self.content = input_text\n",
    "            self.predict = predict\n",
    "            self.indexed_tokens = self.tokenize()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.indexed_tokens)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # remove punctuations\n",
    "        self.content_preprocess = re.sub(\"^[\\uFE70-\\uFEFF]\", \" \", self.content)\n",
    "        self.content_preprocess = re.sub(r\"[.،\\\"()0-9:A-Za-z,!%-/؟'ّ»ـ»'ً«'ُ'ْ'َ'ٍ{}؛'ِ'ٌ…\\\\|\\xad”@_?<>’“\\]\\[éà=‘]\",\"\",self.content_preprocess) \n",
    "        \n",
    "        words=[]\n",
    "        for i in self.content_preprocess.split():\n",
    "            i.strip()\n",
    "            words.append(i)\n",
    "        self.content = (\" \".join(words)).strip()\n",
    "\n",
    "    def tokenize(self):\n",
    "        if self.predict == False:\n",
    "            self.preprocess()\n",
    "        indexed_tokens = tokenizer.encode(self.content)\n",
    "        return indexed_tokens\n",
    "\n",
    "    def tokens_info(self):\n",
    "        print('total tokens: %d' % (len(self.indexed_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMzt5qeNLX8e"
   },
   "outputs": [],
   "source": [
    "class Sequences():\n",
    "    def __init__(self, text_object, max_len, step):\n",
    "        self.tokens_ind = text_object.indexed_tokens\n",
    "        self.max_len = max_len\n",
    "        self.step = step\n",
    "        self.sequences, self.next_subwords = self.create_sequences()\n",
    "  \n",
    "    def __repr__(self):\n",
    "        return 'Sequence object of max_len: %d and step: %d' % (self.max_len, self.step)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def create_sequences(self):\n",
    "        input_sequences = []\n",
    "        for i in range(1, len(self.tokens_ind), self.step):\n",
    "            n_gram_sequence = self.tokens_ind[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "        sequences, next_subwords = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    \n",
    "        return sequences, next_subwords\n",
    "\n",
    "    def sequences_info(self):\n",
    "        print('number of sequences of length %d: %d' % (self.max_len, len(self.sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvVLhNB0L2Aj"
   },
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GjG88UHKrt5"
   },
   "outputs": [],
   "source": [
    "train_sequences=[]\n",
    "train_next_subwords=[]\n",
    "\n",
    "for line in X_tr:\n",
    "    line_encoded = Text(line, tokenizer)\n",
    "    # print(len(line_encoded.tokens))\n",
    "    line_sequences = Sequences(line_encoded, max_len, step)\n",
    "    train_sequences += [seq.tolist() for seq in line_sequences.sequences]\n",
    "    train_next_subwords += line_sequences.next_subwords.tolist()\n",
    "    # line_sequences.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW-uoXv7Bz2J"
   },
   "outputs": [],
   "source": [
    "val_sequences=[]\n",
    "val_next_subwords=[]\n",
    "\n",
    "for line in X_val:\n",
    "    line_encoded = Text(line, tokenizer)\n",
    "    # print(len(line_encoded.tokens))\n",
    "    line_sequences = Sequences(line_encoded, max_len, step)\n",
    "    val_sequences += [seq.tolist() for seq in line_sequences.sequences]\n",
    "    val_next_subwords += line_sequences.next_subwords.tolist()\n",
    "    # line_sequences.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekNodoeuf67l"
   },
   "outputs": [],
   "source": [
    "class TextDataGenerator(keras.utils.all_utils.Sequence):\n",
    "    def __init__(self, sequences, next_subwords, sequence_length, vocab_size, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequences = sequences\n",
    "        self.next_subwords = next_subwords\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sequences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        sequences_batch = [self.sequences[k] for k in indexes]\n",
    "        next_subwords_batch = [self.next_subwords[k] for k in indexes]\n",
    "\n",
    "        X = np.array(sequences_batch)\n",
    "        y = keras.utils.np_utils.to_categorical(next_subwords_batch, num_classes=self.vocab_size)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sequences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KV6zIHrITw9"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "params = {\n",
    "  'sequence_length': max_len,\n",
    "  'vocab_size': tokenizer.vocab_size+1,\n",
    "  'batch_size': batch_size,\n",
    "  'shuffle': True\n",
    "}\n",
    "\n",
    "train_generator = TextDataGenerator(train_sequences, train_next_subwords, **params)\n",
    "val_generator = TextDataGenerator(val_sequences, val_next_subwords, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVLc2Y_IIr2I"
   },
   "outputs": [],
   "source": [
    "def LSTM_model(sequence_length, vocab_size, layer_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length=sequence_length-1, trainable=True))\n",
    "    model.add(LSTM(layer_size))#, recurrent_dropout=0.1, dropout=0.1\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiyl2RstJiHd"
   },
   "outputs": [],
   "source": [
    "model = LSTM_model(max_len, tokenizer.vocab_size+1, 256)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXQCPaxWA6PH"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJqsqE-KKmrA",
    "outputId": "466dacee-76db-4b97-ddb3-0aa5ddc3c2bd"
   },
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=20, callbacks=[checkpoint_callback], validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgndJckCkQ8H"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, seq_length, seed_text, n_subwords):\n",
    "    encoded = Text(seed_text, tokenizer, True).indexed_tokens\n",
    "    # generate a fixed number of subwords\n",
    "    for _ in range(n_subwords):\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded_seq = pad_sequences([encoded], maxlen=seq_length-1, truncating='pre')\n",
    "        # predict subword\n",
    "        predict_x = model.predict(encoded_seq, verbose=0)\n",
    "        yhat=np.argmax(predict_x,axis=1)[0]\n",
    "        encoded.append(yhat)    \n",
    "    out_text = Text(encoded, tokenizer, decode=True).content\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjC0JnlAmWsh",
    "outputId": "43cebe45-0bc6-4de3-d426-574ef0e4d056"
   },
   "outputs": [],
   "source": [
    "txt = 'قررت المحكمة'\n",
    "print(generate_seq(model, max_len, txt, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jQUfreSF5WW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WordLevel-DL-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
