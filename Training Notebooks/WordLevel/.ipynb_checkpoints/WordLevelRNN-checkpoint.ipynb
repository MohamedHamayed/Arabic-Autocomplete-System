{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65nlzAA_Jobr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4yTPmV_JbJi",
    "outputId": "0f592027-6a74-4a92-e08b-87c99a00b184"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DhTU8hvJe-A"
   },
   "outputs": [],
   "source": [
    "X_tr, X_val = train_test_split(df['text'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88ZGl-UIJk4U"
   },
   "outputs": [],
   "source": [
    "class Text:\n",
    "    def __init__(self, input_text, token2ind=None, ind2token=None, predict=False):\n",
    "        self.content = input_text\n",
    "        self.predict = predict\n",
    "        self.tokens, self.tokens_distinct = self.tokenize()\n",
    "\n",
    "        if token2ind != None and ind2token != None:\n",
    "            self.token2ind, self.ind2token = token2ind, ind2token\n",
    "        else:\n",
    "            self.token2ind, self.ind2token = self.create_word_mapping(self.tokens_distinct)\n",
    "\n",
    "        self.tokens_ind = [self.token2ind[token] if token in self.token2ind.keys() else self.token2ind['<| unknown |>'] for token in self.tokens]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.tokens_distinct)\n",
    "  \n",
    "    @staticmethod\n",
    "    def create_word_mapping(values_list):\n",
    "        values_list.append('<| unknown |>')\n",
    "        value2ind = {value: ind for ind, value in enumerate(values_list,1)}\n",
    "        ind2value = dict(enumerate(values_list,1))\n",
    "        return value2ind, ind2value\n",
    "\n",
    "    def preprocess(self):\n",
    "        # remove punctuations\n",
    "        self.content_preprocess = re.sub(\"^[\\uFE70-\\uFEFF]\", \" \", self.content)\n",
    "        self.content_preprocess = re.sub(r\"[.،\\\"()0-9:A-Za-z,!%-/؟'ّ»ـ»'ً«'ُ'ْ'َ'ٍ{}؛'ِ'ٌ…\\\\|\\xad”@_?<>’“\\]\\[éà=‘]\",\"\",self.content_preprocess) \n",
    "        \n",
    "        words=[]\n",
    "        for i in self.content_preprocess.split():\n",
    "            i.strip()\n",
    "            words.append(i)\n",
    "        self.content = (\" \".join(words)).strip()\n",
    "\n",
    "    def tokenize(self):\n",
    "        if self.predict == False:\n",
    "            self.preprocess()\n",
    "        tokens = self.content.split(' ')\n",
    "        return tokens, list(set(tokens))\n",
    "\n",
    "    def tokens_info(self):\n",
    "        print('total tokens: %d, distinct tokens: %d' % (len(self.tokens), len(self.tokens_distinct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIEN2ZrKKgoo",
    "outputId": "c37d2033-ecda-4955-c4b3-33ab43ec58fb"
   },
   "outputs": [],
   "source": [
    "data_text = ''\n",
    "\n",
    "for sentence in X_tr:\n",
    "    if sentence != None:\n",
    "        data_text = data_text + sentence\n",
    "\n",
    "vocab = Text(data_text)\n",
    "vocab.tokens_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMzt5qeNLX8e"
   },
   "outputs": [],
   "source": [
    "class Sequences():\n",
    "    def __init__(self, text_object, max_len, step):\n",
    "        self.tokens_ind = text_object.tokens_ind\n",
    "        self.max_len = max_len\n",
    "        self.step = step\n",
    "        self.sequences, self.next_words = self.create_sequences()\n",
    "  \n",
    "    def __repr__(self):\n",
    "        return 'Sequence object of max_len: %d and step: %d' % (self.max_len, self.step)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def create_sequences(self):\n",
    "        input_sequences = []\n",
    "        for i in range(1, len(self.tokens_ind), self.step):\n",
    "            n_gram_sequence = self.tokens_ind[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "        sequences, next_words = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    \n",
    "        return sequences, next_words\n",
    "\n",
    "    def sequences_info(self):\n",
    "        print('number of sequences of length %d: %d' % (self.max_len, len(self.sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvVLhNB0L2Aj"
   },
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "step = 1\n",
    "token2ind, ind2token = vocab.token2ind, vocab.ind2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GjG88UHKrt5"
   },
   "outputs": [],
   "source": [
    "train_sequences=[]\n",
    "train_next_words=[]\n",
    "\n",
    "for line in X_tr:\n",
    "    line_encoded = Text(line, token2ind, ind2token)\n",
    "    # print(len(line_encoded.tokens))\n",
    "    line_sequences = Sequences(line_encoded, max_len, step)\n",
    "    train_sequences += [seq.tolist() for seq in line_sequences.sequences]\n",
    "    train_next_words += line_sequences.next_words.tolist()\n",
    "    # line_sequences.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW-uoXv7Bz2J"
   },
   "outputs": [],
   "source": [
    "val_sequences=[]\n",
    "val_next_words=[]\n",
    "\n",
    "for line in X_val:\n",
    "    line_encoded = Text(line, token2ind, ind2token)\n",
    "    # print(len(line_encoded.tokens))\n",
    "    line_sequences = Sequences(line_encoded, max_len, step)\n",
    "    val_sequences += [seq.tolist() for seq in line_sequences.sequences]\n",
    "    val_next_words += line_sequences.next_words.tolist()\n",
    "    # line_sequences.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekNodoeuf67l"
   },
   "outputs": [],
   "source": [
    "class TextDataGenerator(keras.utils.all_utils.Sequence):\n",
    "    def __init__(self, sequences, next_words, sequence_length, vocab_size, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequences = sequences\n",
    "        self.next_words = next_words\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sequences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        sequences_batch = [self.sequences[k] for k in indexes]\n",
    "        next_words_batch = [self.next_words[k] for k in indexes]\n",
    "\n",
    "        X = np.array(sequences_batch)\n",
    "        y = keras.utils.np_utils.to_categorical(next_words_batch, num_classes=self.vocab_size)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sequences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KV6zIHrITw9"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "params = {\n",
    "  'sequence_length': max_len,\n",
    "  'vocab_size': len(vocab)+1,\n",
    "  'batch_size': batch_size,\n",
    "  'shuffle': True\n",
    "}\n",
    "\n",
    "train_generator = TextDataGenerator(train_sequences, train_next_words, **params)\n",
    "val_generator = TextDataGenerator(val_sequences, val_next_words, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVLc2Y_IIr2I"
   },
   "outputs": [],
   "source": [
    "def LSTM_model(sequence_length, vocab_size, layer_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 256, input_length=sequence_length-1, trainable=True))\n",
    "    model.add(LSTM(layer_size))#, recurrent_dropout=0.1, dropout=0.1\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiyl2RstJiHd"
   },
   "outputs": [],
   "source": [
    "model = LSTM_model(max_len, len(vocab)+1, 256)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXQCPaxWA6PH"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJqsqE-KKmrA",
    "outputId": "466dacee-76db-4b97-ddb3-0aa5ddc3c2bd"
   },
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=10, callbacks=[checkpoint_callback], validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgndJckCkQ8H"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, seq_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the characters as integers\n",
    "        encoded = Text(in_text, token2ind, ind2token, True).tokens_ind\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length-1, truncating='pre')\n",
    "        # predict word\n",
    "        predict_x = model.predict(encoded, verbose=0)\n",
    "        yhat = np.argmax(predict_x,axis=1)[0]\n",
    "        # reverse map integer to word\n",
    "        word = ' ' + ind2token[yhat]\n",
    "        # append to input\n",
    "        in_text += word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjC0JnlAmWsh",
    "outputId": "43cebe45-0bc6-4de3-d426-574ef0e4d056"
   },
   "outputs": [],
   "source": [
    "txt = 'قررت المحكمة'\n",
    "print(generate_seq(model, max_len, txt, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jQUfreSF5WW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(sequences, next_words, model):\n",
    "    perplex = 0\n",
    "    for index, seq in enumerate(sequences):\n",
    "        predict_x = model.predict([seq], verbose=0)\n",
    "        prob = predict_x[0][next_words[index]]\n",
    "        perplex = perplex + math.log(prob,2)\n",
    "\n",
    "    return math.pow(2, -1*(perplex/len(sequences)))\n",
    "\n",
    "perplexity(val_sequences,val_next_words,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WordLevel-DL-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
